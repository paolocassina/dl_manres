{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e9f6ec1-38a0-4a62-9e42-9b0e581038b0",
   "metadata": {},
   "source": [
    "### Getting the verb lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1051ef72-de4e-4ab9-8d5c-2fe8926149fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_single_token_verbs_verbose(model, tokenizer, verbs):\n",
    "    \"\"\"\n",
    "    Identify which verbs in a list are not tokenized as single tokens by the given model's tokenizer,\n",
    "    and print how each verb is tokenized.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model (not directly used in this function, but included for context).\n",
    "        tokenizer: The tokenizer corresponding to the model.\n",
    "        verbs (list of str): A list of verb lemmas to check.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of verbs that are not tokenized as single tokens.\n",
    "    \"\"\"\n",
    "    non_single_token_verbs = []\n",
    "\n",
    "    for verb in verbs:\n",
    "        # Tokenize the verb\n",
    "        tokenized_verb = tokenizer.tokenize(verb)\n",
    "        \n",
    "        # Print tokenization details\n",
    "        print(f\"Verb: {verb} -> Tokens: {tokenized_verb}\")\n",
    "        \n",
    "        # Check if the verb is split into multiple tokens\n",
    "        if len(tokenized_verb) > 1:\n",
    "            non_single_token_verbs.append(verb)\n",
    "\n",
    "    return non_single_token_verbs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b44a679-f807-4810-87d4-956b426c45c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb: eat -> Tokens: ['eat']\n",
      "Verb: admit -> Tokens: ['ad', 'mit']\n",
      "Verb: approach -> Tokens: ['appro', 'ach']\n",
      "Verb: arrive -> Tokens: ['ar', 'rive']\n",
      "Verb: bash -> Tokens: ['bash']\n",
      "Verb: bellow -> Tokens: ['b', 'ellow']\n",
      "Verb: break -> Tokens: ['break']\n",
      "Verb: clean -> Tokens: ['clean']\n",
      "Verb: clear -> Tokens: ['clear']\n",
      "Verb: come -> Tokens: ['come']\n",
      "Verb: cover -> Tokens: ['cover']\n",
      "Verb: dance -> Tokens: ['d', 'ance']\n",
      "Verb: declare -> Tokens: ['decl', 'are']\n",
      "Verb: destroy -> Tokens: ['destroy']\n",
      "Verb: devour -> Tokens: ['dev', 'our']\n",
      "Verb: die -> Tokens: ['die']\n",
      "Verb: empty -> Tokens: ['empty']\n",
      "Verb: enter -> Tokens: ['enter']\n",
      "Verb: faint -> Tokens: ['f', 'aint']\n",
      "Verb: fall -> Tokens: ['fall']\n",
      "Verb: fill -> Tokens: ['fill']\n",
      "Verb: flutter -> Tokens: ['fl', 'utter']\n",
      "Verb: freeze -> Tokens: ['free', 'ze']\n",
      "Verb: go -> Tokens: ['go']\n",
      "Verb: hit -> Tokens: ['hit']\n",
      "Verb: increase -> Tokens: ['incre', 'ase']\n",
      "Verb: jog -> Tokens: ['j', 'og']\n",
      "Verb: jump -> Tokens: ['jump']\n",
      "Verb: kill -> Tokens: ['kill']\n",
      "Verb: laugh -> Tokens: ['laugh']\n",
      "Verb: melt -> Tokens: ['m', 'elt']\n",
      "Verb: murmur -> Tokens: ['mur', 'mur']\n",
      "Verb: near -> Tokens: ['near']\n",
      "Verb: nibble -> Tokens: ['n', 'ibble']\n",
      "Verb: open -> Tokens: ['open']\n",
      "Verb: pour -> Tokens: ['pour']\n",
      "Verb: proclaim -> Tokens: ['pro', 'claim']\n",
      "Verb: propose -> Tokens: ['pro', 'pose']\n",
      "Verb: remove -> Tokens: ['remove']\n",
      "Verb: rise -> Tokens: ['rise']\n",
      "Verb: roll -> Tokens: ['roll']\n",
      "Verb: rub -> Tokens: ['rub']\n",
      "Verb: run -> Tokens: ['run']\n",
      "Verb: say -> Tokens: ['say']\n",
      "Verb: scour -> Tokens: ['sc', 'our']\n",
      "Verb: scream -> Tokens: ['sc', 'ream']\n",
      "Verb: scribble -> Tokens: ['scrib', 'ble']\n",
      "Verb: scrub -> Tokens: ['sc', 'rub']\n",
      "Verb: shout -> Tokens: ['sh', 'out']\n",
      "Verb: spin -> Tokens: ['spin']\n",
      "Verb: sweep -> Tokens: ['swe', 'ep']\n",
      "Verb: swim -> Tokens: ['sw', 'im']\n",
      "Verb: walk -> Tokens: ['walk']\n",
      "Verb: whisper -> Tokens: ['wh', 'is', 'per']\n",
      "Verb: wipe -> Tokens: ['w', 'ipe']\n",
      "Verb: yell -> Tokens: ['y', 'ell']\n",
      "Verbs not tokenized as single tokens:\n",
      "['admit', 'approach', 'arrive', 'bellow', 'dance', 'declare', 'devour', 'faint', 'flutter', 'freeze', 'increase', 'jog', 'melt', 'murmur', 'nibble', 'proclaim', 'propose', 'scour', 'scream', 'scribble', 'scrub', 'shout', 'sweep', 'swim', 'whisper', 'wipe', 'yell']\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "import re\n",
    "\n",
    "\n",
    "# Load RoBERTa tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
    "model = RobertaForMaskedLM.from_pretrained(\"roberta-large\")\n",
    "\n",
    "# List of verbs\n",
    "fname = \"C:/Users/pcass/manres2vec/data/english_manner_result.csv\"\n",
    "fh = open(fname, \"r\")\n",
    "content = fh.readlines()\n",
    "rootlist = []\n",
    "p = re.compile(\"^(.*?),\")\n",
    "for line in content:\n",
    "    if p.match(line): #for English (no freq counts)\n",
    "        rootlist.append(p.match(line).group(1))\n",
    "# Get verbs that are not tokenized as single words\n",
    "non_single_token_verbs = get_non_single_token_verbs_verbose(model, tokenizer, rootlist)\n",
    "\n",
    "print(\"Verbs not tokenized as single tokens:\")\n",
    "print(non_single_token_verbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86291127-7896-43e0-b65d-45130121cf0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb: eat -> Tokens: ['eat']\n",
      "Verb: admit -> Tokens: ['admit']\n",
      "Verb: approach -> Tokens: ['approach']\n",
      "Verb: arrive -> Tokens: ['arrive']\n",
      "Verb: bash -> Tokens: ['bash']\n",
      "Verb: bellow -> Tokens: ['bell', '##ow']\n",
      "Verb: break -> Tokens: ['break']\n",
      "Verb: clean -> Tokens: ['clean']\n",
      "Verb: clear -> Tokens: ['clear']\n",
      "Verb: come -> Tokens: ['come']\n",
      "Verb: cover -> Tokens: ['cover']\n",
      "Verb: dance -> Tokens: ['dance']\n",
      "Verb: declare -> Tokens: ['declare']\n",
      "Verb: destroy -> Tokens: ['destroy']\n",
      "Verb: devour -> Tokens: ['dev', '##our']\n",
      "Verb: die -> Tokens: ['die']\n",
      "Verb: empty -> Tokens: ['empty']\n",
      "Verb: enter -> Tokens: ['enter']\n",
      "Verb: faint -> Tokens: ['faint']\n",
      "Verb: fall -> Tokens: ['fall']\n",
      "Verb: fill -> Tokens: ['fill']\n",
      "Verb: flutter -> Tokens: ['flutter']\n",
      "Verb: freeze -> Tokens: ['freeze']\n",
      "Verb: go -> Tokens: ['go']\n",
      "Verb: hit -> Tokens: ['hit']\n",
      "Verb: increase -> Tokens: ['increase']\n",
      "Verb: jog -> Tokens: ['jo', '##g']\n",
      "Verb: jump -> Tokens: ['jump']\n",
      "Verb: kill -> Tokens: ['kill']\n",
      "Verb: laugh -> Tokens: ['laugh']\n",
      "Verb: melt -> Tokens: ['melt']\n",
      "Verb: murmur -> Tokens: ['murmur']\n",
      "Verb: near -> Tokens: ['near']\n",
      "Verb: nibble -> Tokens: ['ni', '##bble']\n",
      "Verb: open -> Tokens: ['open']\n",
      "Verb: pour -> Tokens: ['pour']\n",
      "Verb: proclaim -> Tokens: ['pro', '##claim']\n",
      "Verb: propose -> Tokens: ['propose']\n",
      "Verb: remove -> Tokens: ['remove']\n",
      "Verb: rise -> Tokens: ['rise']\n",
      "Verb: roll -> Tokens: ['roll']\n",
      "Verb: rub -> Tokens: ['rub']\n",
      "Verb: run -> Tokens: ['run']\n",
      "Verb: say -> Tokens: ['say']\n",
      "Verb: scour -> Tokens: ['sc', '##our']\n",
      "Verb: scream -> Tokens: ['scream']\n",
      "Verb: scribble -> Tokens: ['sc', '##ri', '##bble']\n",
      "Verb: scrub -> Tokens: ['scrub']\n",
      "Verb: shout -> Tokens: ['shout']\n",
      "Verb: spin -> Tokens: ['spin']\n",
      "Verb: sweep -> Tokens: ['sweep']\n",
      "Verb: swim -> Tokens: ['swim']\n",
      "Verb: walk -> Tokens: ['walk']\n",
      "Verb: whisper -> Tokens: ['whisper']\n",
      "Verb: wipe -> Tokens: ['wipe']\n",
      "Verb: yell -> Tokens: ['yell']\n",
      "BERT - Verbs not tokenized as single tokens:\n",
      "['bellow', 'devour', 'jog', 'nibble', 'proclaim', 'scour', 'scribble']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer_bert = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "model_bert = BertForMaskedLM.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "# List of verbs\n",
    "\n",
    "# Check which verbs are not tokenized as single tokens by BERT\n",
    "non_single_token_verbs_bert = get_non_single_token_verbs_verbose(model_bert, tokenizer_bert, rootlist)\n",
    "\n",
    "print(\"BERT - Verbs not tokenized as single tokens:\")\n",
    "print(non_single_token_verbs_bert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37abc1ea-e53e-46ef-8dc7-475f61771d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb: ate -> Tokens: ['ate']\n",
      "Verb: admitted -> Tokens: ['admitted']\n",
      "Verb: approached -> Tokens: ['approached']\n",
      "Verb: arrived -> Tokens: ['arrived']\n",
      "Verb: bashed -> Tokens: ['bash', '##ed']\n",
      "Verb: bellowed -> Tokens: ['bellowed']\n",
      "Verb: broke -> Tokens: ['broke']\n",
      "Verb: cleaned -> Tokens: ['cleaned']\n",
      "Verb: cleared -> Tokens: ['cleared']\n",
      "Verb: came -> Tokens: ['came']\n",
      "Verb: covered -> Tokens: ['covered']\n",
      "Verb: danced -> Tokens: ['danced']\n",
      "Verb: declared -> Tokens: ['declared']\n",
      "Verb: destroyed -> Tokens: ['destroyed']\n",
      "Verb: devoured -> Tokens: ['dev', '##oured']\n",
      "Verb: died -> Tokens: ['died']\n",
      "Verb: emptied -> Tokens: ['emptied']\n",
      "Verb: entered -> Tokens: ['entered']\n",
      "Verb: fainted -> Tokens: ['faint', '##ed']\n",
      "Verb: fell -> Tokens: ['fell']\n",
      "Verb: filled -> Tokens: ['filled']\n",
      "Verb: fluttered -> Tokens: ['fluttered']\n",
      "Verb: froze -> Tokens: ['froze']\n",
      "Verb: went -> Tokens: ['went']\n",
      "Verb: hit -> Tokens: ['hit']\n",
      "Verb: increased -> Tokens: ['increased']\n",
      "Verb: jogged -> Tokens: ['jogged']\n",
      "Verb: jumped -> Tokens: ['jumped']\n",
      "Verb: killed -> Tokens: ['killed']\n",
      "Verb: laughed -> Tokens: ['laughed']\n",
      "Verb: melted -> Tokens: ['melted']\n",
      "Verb: murmured -> Tokens: ['murmured']\n",
      "Verb: neared -> Tokens: ['neared']\n",
      "Verb: nibbled -> Tokens: ['ni', '##bbled']\n",
      "Verb: opened -> Tokens: ['opened']\n",
      "Verb: poured -> Tokens: ['poured']\n",
      "Verb: proclaimed -> Tokens: ['proclaimed']\n",
      "Verb: proposed -> Tokens: ['proposed']\n",
      "Verb: removed -> Tokens: ['removed']\n",
      "Verb: rose -> Tokens: ['rose']\n",
      "Verb: rolled -> Tokens: ['rolled']\n",
      "Verb: rubbed -> Tokens: ['rubbed']\n",
      "Verb: ran -> Tokens: ['ran']\n",
      "Verb: said -> Tokens: ['said']\n",
      "Verb: scoured -> Tokens: ['sc', '##oured']\n",
      "Verb: screamed -> Tokens: ['screamed']\n",
      "Verb: scribbled -> Tokens: ['sc', '##ri', '##bbled']\n",
      "Verb: scrubbed -> Tokens: ['scrubbed']\n",
      "Verb: shouted -> Tokens: ['shouted']\n",
      "Verb: spun -> Tokens: ['spun']\n",
      "Verb: swept -> Tokens: ['swept']\n",
      "Verb: swam -> Tokens: ['swam']\n",
      "Verb: walked -> Tokens: ['walked']\n",
      "Verb: whispered -> Tokens: ['whispered']\n",
      "Verb: wiped -> Tokens: ['wiped']\n",
      "Verb: yelled -> Tokens: ['yelled']\n",
      "BERT - Verbs not tokenized as single tokens:\n",
      "['bashed', 'devoured', 'fainted', 'nibbled', 'scoured', 'scribbled']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import re\n",
    "fname = \"C:/Users/pcass/manres2vec/data/english_manres_past_tense.csv\"\n",
    "fh = open(fname, \"r\")\n",
    "content = fh.readlines()\n",
    "rootlist = []\n",
    "p = re.compile(\"^(.*?),\")\n",
    "for line in content:\n",
    "    if p.match(line): #for English (no freq counts)\n",
    "        rootlist.append(p.match(line).group(1))\n",
    "\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "del rootlist[0]\n",
    "rootlist.insert(0, 'ate')\n",
    "tokenizer_bert = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "model_bert = BertForMaskedLM.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "# List of verbs\n",
    "\n",
    "# Check which verbs are not tokenized as single tokens by BERT\n",
    "non_single_token_verbs_bert = get_non_single_token_verbs_verbose(model_bert, tokenizer_bert, rootlist)\n",
    "\n",
    "print(\"BERT - Verbs not tokenized as single tokens:\")\n",
    "print(non_single_token_verbs_bert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "891a5882-eb0d-41ed-ab82-c7747eb26cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pcass\\anaconda3\\envs\\conda_env_language\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb: ate -> Tokens: ['ate']\n",
      "Verb: admitted -> Tokens: ['admitted']\n",
      "Verb: approached -> Tokens: ['approached']\n",
      "Verb: arrived -> Tokens: ['arrived']\n",
      "Verb: bellowed -> Tokens: ['bellowed']\n",
      "Verb: broke -> Tokens: ['broke']\n",
      "Verb: cleaned -> Tokens: ['cleaned']\n",
      "Verb: crawled -> Tokens: ['crawled']\n",
      "Verb: cleared -> Tokens: ['cleared']\n",
      "Verb: came -> Tokens: ['came']\n",
      "Verb: covered -> Tokens: ['covered']\n",
      "Verb: danced -> Tokens: ['danced']\n",
      "Verb: declared -> Tokens: ['declared']\n",
      "Verb: destroyed -> Tokens: ['destroyed']\n",
      "Verb: died -> Tokens: ['died']\n",
      "Verb: emptied -> Tokens: ['emptied']\n",
      "Verb: entered -> Tokens: ['entered']\n",
      "Verb: fell -> Tokens: ['fell']\n",
      "Verb: filled -> Tokens: ['filled']\n",
      "Verb: fluttered -> Tokens: ['fluttered']\n",
      "Verb: froze -> Tokens: ['froze']\n",
      "Verb: went -> Tokens: ['went']\n",
      "Verb: hopped -> Tokens: ['hopped']\n",
      "Verb: hit -> Tokens: ['hit']\n",
      "Verb: increased -> Tokens: ['increased']\n",
      "Verb: jogged -> Tokens: ['jogged']\n",
      "Verb: jumped -> Tokens: ['jumped']\n",
      "Verb: killed -> Tokens: ['killed']\n",
      "Verb: laughed -> Tokens: ['laughed']\n",
      "Verb: melted -> Tokens: ['melted']\n",
      "Verb: neared -> Tokens: ['neared']\n",
      "Verb: opened -> Tokens: ['opened']\n",
      "Verb: poured -> Tokens: ['poured']\n",
      "Verb: proclaimed -> Tokens: ['proclaimed']\n",
      "Verb: proposed -> Tokens: ['proposed']\n",
      "Verb: removed -> Tokens: ['removed']\n",
      "Verb: rose -> Tokens: ['rose']\n",
      "Verb: rolled -> Tokens: ['rolled']\n",
      "Verb: rubbed -> Tokens: ['rubbed']\n",
      "Verb: ran  -> Tokens: ['ran']\n",
      "Verb: said -> Tokens: ['said']\n",
      "Verb: screamed -> Tokens: ['screamed']\n",
      "Verb: shattered -> Tokens: ['shattered']\n",
      "Verb: scrubbed -> Tokens: ['scrubbed']\n",
      "Verb: shook -> Tokens: ['shook']\n",
      "Verb: shouted -> Tokens: ['shouted']\n",
      "Verb: spun -> Tokens: ['spun']\n",
      "Verb: swept -> Tokens: ['swept']\n",
      "Verb: swam  -> Tokens: ['swam']\n",
      "Verb: walked -> Tokens: ['walked']\n",
      "Verb: whispered -> Tokens: ['whispered']\n",
      "Verb: wiped -> Tokens: ['wiped']\n",
      "Verb: yelled -> Tokens: ['yelled']\n",
      "Verb: devoured -> Tokens: ['dev', '##oured']\n",
      "BERT - Verbs not tokenized as single tokens:\n",
      "['devoured']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import re\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"C:/Users/pcass/dl_manres/data/manres_verbs.csv\")\n",
    "rootlist = list(df['verb'])\n",
    "# fname = \"C:/Users/pcass/dl_manres/data/manres_verbs.csv\"\n",
    "# fh = open(fname, \"r\")\n",
    "# content = fh.readlines()\n",
    "# rootlist = []\n",
    "# p = re.compile(\"^(.*?),\")\n",
    "# for line in content:\n",
    "#     if p.match(line): #for English (no freq counts)\n",
    "#         rootlist.append(p.match(line).group(1))\n",
    "\n",
    "#print(rootlist)\n",
    "# Load BERT tokenizer and model\n",
    "#del rootlist[0]\n",
    "#rootlist.insert(0, 'ate')\n",
    "tokenizer_bert = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "model_bert = BertForMaskedLM.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "# List of verbs\n",
    "\n",
    "# Check which verbs are not tokenized as single tokens by BERT\n",
    "non_single_token_verbs_bert = get_non_single_token_verbs_verbose(model_bert, tokenizer_bert, rootlist)\n",
    "\n",
    "print(\"BERT - Verbs not tokenized as single tokens:\")\n",
    "print(non_single_token_verbs_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1415df42-9420-46fb-b8d7-1bb0c97492c1",
   "metadata": {},
   "source": [
    "### Identifying word senses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6556ae86-68f1-4eea-a053-8548f80be1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terminate\n",
      "['She interrupted her pregnancy', 'break a lucky streak', 'break the cycle of poverty']\n"
     ]
    }
   ],
   "source": [
    "s = 'break.v.1'\n",
    "print(wn.synset(s).definition())\n",
    "print(wn.synset(s).examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "21e73ad4-2d58-4dcf-9c42-2075d69f4c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shout.v.02: utter a sudden loud cry\n",
      "Examples: ['she cried with pain when the doctor inserted the needle', \"I yelled to her from the window but she couldn't hear me\"]\n",
      "\n",
      "yell.v.02: utter or declare in a very loud voice\n",
      "Examples: [\"You don't have to yell--I can hear you just fine\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "verb = wn.synsets('yell', pos=wn.VERB)  # Get all verb senses of 'break'\n",
    "for sense in verb:\n",
    "    #if 'break' in sense.name():\n",
    "     print(f\"{sense.name()}: {sense.definition()}\")\n",
    "     print(f\"Examples: {sense.examples()}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d0039b6-c5a5-4540-aa38-d2c2dc1d8661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat%2:37:00::\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Lemma('eat.v.04.eat')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eat = wn.lemma('eat.v.0.eat')\n",
    "print(eat.key())\n",
    "wn.lemma_from_key(eat.key())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b43fdab0-b29c-4b25-b87d-651adf48239e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sense: break.v.02 - become separated into pieces or fragments\n",
      "Examples:\n",
      "  - The figurine broke\n",
      "  - The freshly baked loaf fell apart\n",
      "\n",
      "\n",
      "Sense: break.v.03 - render inoperable or ineffective\n",
      "Examples:\n",
      "  - You broke the alarm clock when you took it apart!\n",
      "\n",
      "\n",
      "Sense: break.v.04 - ruin completely\n",
      "Examples:\n",
      "  - He busted my radio!\n",
      "\n",
      "\n",
      "Sense: break.v.05 - destroy the integrity of; usually by force; cause to separate into pieces or fragments\n",
      "Examples:\n",
      "  - He broke the glass plate\n",
      "  - She broke the match\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# selected senses\n",
    "target_senses = ['break.v.02', 'break.v.03', 'break.v.04', 'break.v.05', 'break.v.01', 'break.v.15']\n",
    "\n",
    "# Retrieve all verb senses for \"break\"\n",
    "verb_senses = wn.synsets('break', pos=wn.VERB)\n",
    "\n",
    "# Iterate through the senses and match with the target senses\n",
    "for sense in verb_senses:\n",
    "    if sense.name() in target_senses:\n",
    "        print(f\"Sense: {sense.name()} - {sense.definition()}\")\n",
    "        print(\"Examples:\")\n",
    "        for example in sense.examples():\n",
    "            print(f\"  - {example}\")\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35af018-1db8-4cdb-a325-1de3b9ec338f",
   "metadata": {},
   "source": [
    "### Create mini-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0f8e5ffc-4dcb-4949-8491-585e77428e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: The Fulton County Grand Jury said Friday an investigation of Atlanta 's recent primary election produced `` no evidence '' that any irregularities took place .\n",
      "Sentence 2: The jury further said in term end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted .\n",
      "Sentence 3: `` Only a relative handful of such reports was received '' , the jury said , `` considering the widespread interest in the election , the number of voters and the size of this city '' .\n",
      "Sentence 4: The jury said it did find that many of Georgia 's registration and election laws `` are outmoded or inadequate and often ambiguous '' .\n",
      "Sentence 5: The grand jury commented on a number of other topics , among them the Atlanta and Fulton County purchasing departments which it said `` are well operated and follow generally accepted practices which inure to the best interest of both governments '' .\n",
      "Sentence 6: However , the jury said it believes `` these two offices should be combined to achieve greater efficiency and reduce the cost of administration '' .\n",
      "Sentence 7: The City Purchasing Department , the jury said , `` is lacking in experienced clerical personnel as a result of city personnel policies '' .\n",
      "Sentence 8: `` This is one of the major items in the Fulton County general assistance program '' , the jury said , but the State Welfare Department `` has seen fit to distribute these funds through the welfare departments of all the counties in the state with the exception of Fulton County , which receives none of this money .\n",
      "Sentence 9: The jurors said they realize `` a proportionate distribution of these funds might disable this program in our less populous counties '' .\n",
      "Sentence 10: Nevertheless , `` we feel that in the future Fulton County should receive some portion of these available funds '' , the jurors said .\n",
      "Sentence 11: The jury said it found the court `` has incorporated into its operating procedures the recommendations '' of two previous grand juries , the Atlanta Bar Association and an interim citizens committee .\n",
      "Sentence 12: `` These actions should serve to protect in fact and in effect the court 's wards from undue costs and its appointed and elected servants from unmeritorious criticisms '' , the jury said .\n",
      "Sentence 13: Attorneys for the mayor said that an amicable property settlement has been agreed upon .\n",
      "Sentence 14: The petition said that the couple has not lived together as man and wife for more than a year .\n",
      "Sentence 15: Georgia Republicans are getting strong encouragement to enter a candidate in the 1962 governor 's race , a top official said Wednesday .\n",
      "Sentence 16: Robert Snodgrass , state GOP chairman , said a meeting held Tuesday night in Blue Ridge brought enthusiastic responses from the audience .\n",
      "Sentence 17: The largest hurdle the Republicans would have to face is a state law which says that before making a first race , one of two alternative courses must be taken :\n",
      "Sentence 18: A Highway Department source said there also is a plan there to issue some $ 3 million to $ 4 million worth of Rural Roads Authority bonds for rural road construction work .\n",
      "Sentence 19: Pelham said Sunday night there was research being done on whether the `` quickie '' vote on the increase can be repealed outright or whether notice would have to first be given that reconsideration of the action would be sought .\n",
      "Sentence 20: While emphasizing that technical details were not fully worked out , Pelham said his resolution would seek to set aside the privilege resolution which the House voted through 87 - 31 .\n",
      "Sentence 21: Barber , who is in his 13th year as a legislator , said there `` are some members of our congressional delegation in Washington who would like to see it ( the resolution ) passed '' .\n",
      "Sentence 22: It says that `` in the event Congress does provide this increase in federal funds '' , the State Board of Education should be directed to `` give priority '' to teacher pay raises .\n",
      "Sentence 23: `` This was the coolest , calmest election I ever saw '' , Colquitt Policeman Tom Williams said .\n",
      "Sentence 24: Ordinary Williams said he , too , was subjected to anonymous calls soon after he scheduled the election .\n",
      "Sentence 25: Sheriff Felix Tabb said the ordinary apparently made good his promise .\n",
      "Sentence 26: `` Everything went real smooth '' , the sheriff said .\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def find_sentences_with_sense(file_path, target_word, target_wnsns, pos_tag='VB'):\n",
    "    \"\"\"\n",
    "    Search a SemCor file for sentences containing a specific word, part of speech, and WordNet sense.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the SemCor file.\n",
    "        target_word (str): The token to search for (e.g., \"broke\").\n",
    "        pos_tag (str): The part of speech tag (e.g., \"VB\" for verbs).\n",
    "        target_wnsns (list): The target WordNet senses number (e.g., \"1\" for sense 01).\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of sentences (str) containing the target word with the specified POS and sense.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        current_sentence = \"\"\n",
    "        inside_sentence = False\n",
    "        for line in file:\n",
    "            # Check if this is the start of a sentence\n",
    "            if re.match(r\"<s\\s+snum=\\d+>\", line):\n",
    "                inside_sentence = True\n",
    "                current_sentence = \"\"\n",
    "\n",
    "            # Check if this is the end of a sentence\n",
    "            elif \"</s>\" in line:\n",
    "                inside_sentence = False\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence.strip())\n",
    "                current_sentence = \"\"\n",
    "\n",
    "            # If inside a sentence, add the line to the current sentence\n",
    "            elif inside_sentence:\n",
    "                current_sentence += line.strip() + \" \"\n",
    "\n",
    "    # Filter sentences containing the target word, pos, and sense\n",
    "    matching_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # Use a regex to find words matching the target criteria\n",
    "        senses_pattern = \"|\".join(map(re.escape, target_wnsns))\n",
    "        pattern = re.compile(\n",
    "            rf'<wf[^>]*pos={pos_tag}[^>]*lemma={target_word}[^>]*wnsn=({senses_pattern})[^>]*>[^<]+</wf>'\n",
    "        )\n",
    "        if pattern.search(sentence):\n",
    "            # Remove XML tags to extract the plain text sentence\n",
    "            plain_sentence = re.sub(r\"<[^>]+>\", \"\", sentence)\n",
    "            plain_sentence = plain_sentence.replace(\"_\", \" \")\n",
    "            matching_sentences.append(plain_sentence.strip())\n",
    "    \n",
    "    return matching_sentences\n",
    "\n",
    "# Example usage\n",
    "file_path = \"../data/corpus/semcor3.0/brown1/tagfiles/br-a01\"\n",
    "target_word = \"say\"\n",
    "pos_tag = \"VB\"\n",
    "target_wnsns = ['1','2','3']\n",
    "\n",
    "sentences = find_sentences_with_sense(file_path, target_word, target_wnsns, pos_tag)\n",
    "for i, sent in enumerate(sentences):\n",
    "    print(f\"Sentence {i + 1}: {sent}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "870fcd40-97f5-493e-ba33-266dc258603d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 113 sentences.\n",
      "If the Orioles are to break their losing streak within the next two days , it will have to be at the expense of the American League champion New York Yankees , who come in here tomorrow for a night game and a single test Sunday afternoon .\n",
      "He hopes to melt off an additional eight pounds before the Flock breaks camp three weeks hence .\n",
      "The crowd at the twenty-first annual K. of C. Games , final indoor meet of the season , got a thrill a few minutes earlier when a slender , bespectacled woman broke the one week old world record in the half-mile run .\n",
      "He broke that boy ( Air Force fullback Nick Arshinkoff ) in two and knocked him loose from the football '' .\n",
      "`` He timed it just right and broke through there before the boy ( halfback Terry Isaacson ) had time to turn around .\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def gather_sentences_with_sense(base_dir, target_word, target_wnsns,  pos_tag='VB'):\n",
    "    \"\"\"\n",
    "    Gathers all sentences containing a specific word with a specific WordNet sense\n",
    "    across all files in the semcor3.0 directory.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): Path to the semcor3.0 directory.\n",
    "        pos_tag (str): Part-of-speech tag (e.g., \"VB\").\n",
    "        target_wnsn (str): Target WordNet sense number (e.g., \"1\").\n",
    "        target_word (str): The exact token to search for (e.g., \"broke\").\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of all sentences that match the criteria.\n",
    "    \"\"\"\n",
    "    all_sentences = []\n",
    "\n",
    "    # Traverse the subdirectories (brown1, brown2, brownv)\n",
    "    for folder in [\"brown1\", \"brown2\", \"brownv\"]:\n",
    "        tagfiles_dir = os.path.join(base_dir, folder, \"tagfiles\")\n",
    "\n",
    "        # Ensure the tagfiles directory exists\n",
    "        if not os.path.exists(tagfiles_dir):\n",
    "            print(f\"Directory not found: {tagfiles_dir}\")\n",
    "            continue\n",
    "\n",
    "        # Iterate through all files in the tagfiles directory\n",
    "        for filename in os.listdir(tagfiles_dir):\n",
    "            file_path = os.path.normpath(os.path.join(tagfiles_dir, filename))\n",
    "\n",
    "            # Skip if not a file\n",
    "            if not os.path.isfile(file_path):\n",
    "                continue\n",
    "\n",
    "            # Call the find_sentences_with_sense function and extend the results\n",
    "            try:\n",
    "                sentences = find_sentences_with_sense(file_path, target_word, target_wnsns, pos_tag)\n",
    "                all_sentences.extend(sentences)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "    return all_sentences\n",
    "\n",
    "\n",
    "# Base directory containing the semcor3.0 corpus\n",
    "base_dir = \"../data/corpus/semcor3.0\"\n",
    "\n",
    "# Parameters for the search\n",
    "pos_tag = \"VB\"        # Part-of-speech tag (e.g., verb)\n",
    "target_wnsns = ['1','2','3']     # WordNet sense number\n",
    "target_word = \"break\" # The exact word token to match\n",
    "\n",
    "# Call the function to gather all matching sentences\n",
    "#find_sentences_with_sense(file_path, target_word, pos_tag, target_wnsn)\n",
    "sentences = gather_sentences_with_sense(base_dir, target_word, target_wnsns)\n",
    "\n",
    "# Output the number of sentences found and some examples\n",
    "print(f\"Found {len(sentences)} sentences.\")\n",
    "for sentence in sentences[:5]:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b065e49f-23f2-499c-a245-23c73d9e93c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_examples_from_file(filepath):\n",
    "\n",
    "    df_senses = pd.read_csv(filepath, dtype='str')\n",
    "    exs_from_verb = {}\n",
    "    \n",
    "    for i, row in df_senses.iterrows():\n",
    "        verb = row['verb']\n",
    "        senses = list(row[['sense_1', 'sense_2', 'sense_3', 'sense_4', 'sense_5']].dropna())\n",
    "        sentences = gather_sentences_with_sense(base_dir=\"../data/corpus/semcor3.0\", target_word=verb, target_wnsns=senses)\n",
    "        exs_from_verb[verb] = sentences\n",
    "\n",
    "    \n",
    "    df_examples = pd.DataFrame.from_dict(exs_from_verb, orient='index')\n",
    "    \n",
    "    max_columns = df_examples.shape[1]\n",
    "    column_names = [\"example_\" + str(i) for i in range(max_columns)]\n",
    "\n",
    "    df_examples\n",
    "    df_examples.fillna(\"\", inplace=True)\n",
    "\n",
    "    \n",
    "    df_examples.columns = column_names\n",
    "\n",
    "    df_examples.to_csv(\"../data/diagnostics/example_usage/manres_examples.csv\", index=True)\n",
    "\n",
    "    return df_examples\n",
    "\n",
    "\n",
    "df_examples = get_examples_from_file(\"../data/manres_senses.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "56e1d961-8d97-4ad4-af29-03de53c19c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eat           73\n",
       "admit         55\n",
       "approach      44\n",
       "arrive        70\n",
       "bellow         4\n",
       "break         67\n",
       "clean         34\n",
       "crawl         25\n",
       "clear         28\n",
       "come         642\n",
       "cover         56\n",
       "dance         28\n",
       "declare       61\n",
       "destroy       67\n",
       "die          124\n",
       "empty          9\n",
       "enter        145\n",
       "fall         106\n",
       "fill          77\n",
       "flutter        0\n",
       "freeze        14\n",
       "go           627\n",
       "hop          100\n",
       "hit           43\n",
       "increase     135\n",
       "jog            0\n",
       "jump          23\n",
       "kill          81\n",
       "laugh         53\n",
       "melt           5\n",
       "near          10\n",
       "open          82\n",
       "pour          25\n",
       "proclaim      24\n",
       "propose       47\n",
       "remove        95\n",
       "rise          72\n",
       "roll          22\n",
       "rub           15\n",
       "run          175\n",
       "say         1888\n",
       "scream        19\n",
       "shatter       10\n",
       "scrub          4\n",
       "shake         65\n",
       "shout         48\n",
       "spin          13\n",
       "sweep         20\n",
       "swim          12\n",
       "walk         180\n",
       "whisper       18\n",
       "wipe          20\n",
       "yell          16\n",
       "dtype: int64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_examples.iloc[:, 1:].apply(lambda row: row.astype(bool).sum(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "38779087-e527-484d-954f-48a683d84392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Reference                                               Left      KWIC  \\\n",
      "0     text#86  , and the highlight of our act was a handbag c...     empty   \n",
      "1    text#141  are very bad, and it was considered more appro...  emptying   \n",
      "2    text#223  a certain difficulty, yes. In fact, I''m... of...  emptying   \n",
      "3    text#496  . Oh, yes, there was this guy from the Salvati...     empty   \n",
      "4    text#508  in the left eye. It''s very complicated. I mea...     empty   \n",
      "5    text#534  merchant one of those small miniature bottles ...   emptied   \n",
      "6    text#566  , the lot. So why did you tell them to... beca...   emptied   \n",
      "7    text#566  to... because I was really excited about havin...   emptied   \n",
      "8    text#573  rich girls. And they''re there on the weekends...  emptying   \n",
      "9    text#584  aeroplane out of graphene composites... and th...     empty   \n",
      "10   text#630  an enemy plane once, didn''t you? I pursued an...   emptied   \n",
      "11   text#682  president. I take the number six bus to the Le...  emptying   \n",
      "12   text#882  advisor as the Mary Rose was being lifted from...   emptied   \n",
      "13   text#886  . Yes. Well, I thought it was about people who...   emptied   \n",
      "14   text#959  the rock and roll stadium life of stand-up tou...  emptying   \n",
      "15   text#974  is tragic. And in fact, of course, Crowhurst n...     empty   \n",
      "16   text#987  newspaper, which was a newspaper called The Da...     empty   \n",
      "17  text#1002  said, ''''I know you''re doing these sessions....     empty   \n",
      "18  text#1004  , you see. I was going home after the second b...  emptying   \n",
      "19  text#1008  you first did a shift in the pub. Seven. What ...  emptying   \n",
      "20  text#1084  And I think it''s important if you''re using s...     empty   \n",
      "21  text#1244  you''re starring in the West End now, getting ...  emptying   \n",
      "22  text#1328  one, which changed my life, was a scene... whe...     empty   \n",
      "23  text#1368  backwards and forwards to London. And when I w...   emptied   \n",
      "24  text#1368  in my career. I don''t know about yours. Becau...   emptied   \n",
      "25  text#1368  that was the year of the Jubilee Drives, King ...   emptied   \n",
      "26  text#1506  . A bit famous and empty. What are you seeing,...     empty   \n",
      "27  text#1579  stood at the end of Market Street, watched thi...   empties   \n",
      "28  text#1580  estate going. And there are some tragic houses...   emptied   \n",
      "29  text#1582  friends and neighbours and I love having peopl...     empty   \n",
      "30  text#1582  that I always call a freezer party. I empty my...  emptying   \n",
      "31  text#1647  ? Well, pretty poor. My mother raised my broth...     empty   \n",
      "32  text#1653  be that baker''s boy. I''ll always be collecti...   empties   \n",
      "33  text#1702  this absolute monster that would come out at n...   emptied   \n",
      "34  text#1713  and Jonathan. And one of the verses in the son...     empty   \n",
      "35  text#1772  who directs says he calls it a period of re-en...  emptying   \n",
      "36  text#1800  I got two quid. Let''s have some more music. W...   empties   \n",
      "37  text#1891  room that morning and And the place was smelli...   emptied   \n",
      "38  text#2008  it was a two-storey Welsh cottage. The front r...   emptied   \n",
      "39  text#2017  inside loo. It had a bucket in a shed across t...     empty   \n",
      "40  text#2036  possibilities for the scientists. Yes. You''ve...     empty   \n",
      "41  text#2102  would get up and leave. They said, my God, tha...  Emptying   \n",
      "42  text#2113  but if you took back the coca-cola bottle you ...   empties   \n",
      "43  text#2160  . And on the other, it said on all of them, Re...   emptied   \n",
      "44  text#2259  We talked about it the night before the race a...     empty   \n",
      "45  text#2302  knew better because I was too young and unexpe...   emptied   \n",
      "46  text#2332  you''re allowed a luxury, too. What will your ...     empty   \n",
      "47  text#2332  . A variety. A full bar. Exactly. We''ll suppl...     empty   \n",
      "48  text#2361  left on the circuit after 10 hours of racing. ...     Empty   \n",
      "49  text#2373  these civil wars, which were the cause of so m...     empty   \n",
      "\n",
      "                                                Right  \n",
      "0   out the contents. I still do it now from time ...  \n",
      "1   trucks of rock and shale and carbide and thing...  \n",
      "2   the waste paper baskets. Yes, home is importan...  \n",
      "3   the pubs on a Saturday night. LAUGHTER But the...  \n",
      "4   myself, become a channel for the truth of the ...  \n",
      "5   into my coffee cup in the Lyons tea shop at ab...  \n",
      "6   that. They could have just emptied that, but t...  \n",
      "7   that, but then if I just get them to do it all...  \n",
      "8   the ashtrays. I try to move among them like a ...  \n",
      "9   . It also turns out you can build transistors ...  \n",
      "10  my entire magazine at a very low-flying German...  \n",
      "11  the pans. And I don''t curse or spit or kick a...  \n",
      "12  her first, you know. Right. And Margaret Rule,...  \n",
      "13  every theatre. I mean, the only thing I could ...  \n",
      "14  the bins? Is that tricky? Yeah, it is a bit tr...  \n",
      "15  , yes. Tell me then about the voyage a little,...  \n",
      "16  the water pot of the cartoonist, who was a man...  \n",
      "17  it, walk out the main doors, walk across and y...  \n",
      "18  his pockets, oozing ten-pound notes and sovere...  \n",
      "19  ashtrays, cleaning tables. And you''ve said th...  \n",
      "20  it. You''re kind of appealing to the scrubber ...  \n",
      "21  the bins the rest of the time at home with the...  \n",
      "22  their pockets of everything and leave it all b...  \n",
      "23  . Whereas, of course, I''d really want to be d...  \n",
      "24  the theatres. Something certainly emptied ours...  \n",
      "25  ours. It did, too. And so I had to look around...  \n",
      "26  ... What are you saying? I''m shallow and empt...  \n",
      "27  , you know, and go away. And I''d finish my sa...  \n",
      "28  of the furniture, for example, and the decorat...  \n",
      "29  my freezer and just cook what''s in it and I''...  \n",
      "30  the freezer, come round anyone. But do they as...  \n",
      "31  , but there was no indoor plumbing. It was a p...  \n",
      "32  , and taking them up the thing, collecting the...  \n",
      "33  all 12 bottles of scotch down the sink that he...  \n",
      "34  , how brief I now perceive joys to be. CHOIR S...  \n",
      "35  the bins and filling the fridge. Re-entry is a...  \n",
      "36  the room. If it was up to me, I''m a fan of wh...  \n",
      "37  , I think, for something like, certainly for m...  \n",
      "38  in the stream that ran by the door. And we liv...  \n",
      "39  the bucket in a hole dug next to the stream in...  \n",
      "40  the oceans of fish. Well, you know, when we go...  \n",
      "41  a restaurant. Welcome everywhere. Very good, R...  \n",
      "42  and you know i used to go out with 10 shilling...  \n",
      "43  it. So now you''re off again. Where are you go...  \n",
      "44  ourselves of everything. And our finish line w...  \n",
      "45  me of all joy of writing, so I stopped. And I ...  \n",
      "46  . Right. Full bottles of what? Of vodka, wine,...  \n",
      "47  them and give me plenty of paper. Not only cou...  \n",
      "48  now, but for those hardened enthusiasts huddle...  \n",
      "49  the pots because we didn''t have an inside lav...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/corpus/desert_island_discs_corpus/empty_concordance_desert_island_discs_20250128190117.csv')\n",
    "\n",
    "def combine_text_columns(df):\n",
    "    \"\"\"\n",
    "    Combine the 'Left', 'KWIC', and 'Right' columns of a DataFrame into a new column 'Full_Sentence'.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame with 'Left', 'KWIC', and 'Right' columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with a new column 'Full_Sentence' added.\n",
    "    \"\"\"\n",
    "    # Combine the columns with a space in between\n",
    "    df['Example'] = df['Left'].str.strip() + \" \" + df['KWIC'].str.strip() + \" \" + df['Right'].str.strip()\n",
    "    return df\n",
    "\n",
    "# Combine text into the new column\n",
    "#df = combine_text_columns(df)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
