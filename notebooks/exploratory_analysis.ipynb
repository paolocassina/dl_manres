{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1051ef72-de4e-4ab9-8d5c-2fe8926149fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_single_token_verbs_verbose(model, tokenizer, verbs):\n",
    "    \"\"\"\n",
    "    Identify which verbs in a list are not tokenized as single tokens by the given model's tokenizer,\n",
    "    and print how each verb is tokenized.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model (not directly used in this function, but included for context).\n",
    "        tokenizer: The tokenizer corresponding to the model.\n",
    "        verbs (list of str): A list of verb lemmas to check.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of verbs that are not tokenized as single tokens.\n",
    "    \"\"\"\n",
    "    non_single_token_verbs = []\n",
    "\n",
    "    for verb in verbs:\n",
    "        # Tokenize the verb\n",
    "        tokenized_verb = tokenizer.tokenize(verb)\n",
    "        \n",
    "        # Print tokenization details\n",
    "        print(f\"Verb: {verb} -> Tokens: {tokenized_verb}\")\n",
    "        \n",
    "        # Check if the verb is split into multiple tokens\n",
    "        if len(tokenized_verb) > 1:\n",
    "            non_single_token_verbs.append(verb)\n",
    "\n",
    "    return non_single_token_verbs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b44a679-f807-4810-87d4-956b426c45c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb: eat -> Tokens: ['eat']\n",
      "Verb: admit -> Tokens: ['ad', 'mit']\n",
      "Verb: approach -> Tokens: ['appro', 'ach']\n",
      "Verb: arrive -> Tokens: ['ar', 'rive']\n",
      "Verb: bash -> Tokens: ['bash']\n",
      "Verb: bellow -> Tokens: ['b', 'ellow']\n",
      "Verb: break -> Tokens: ['break']\n",
      "Verb: clean -> Tokens: ['clean']\n",
      "Verb: clear -> Tokens: ['clear']\n",
      "Verb: come -> Tokens: ['come']\n",
      "Verb: cover -> Tokens: ['cover']\n",
      "Verb: dance -> Tokens: ['d', 'ance']\n",
      "Verb: declare -> Tokens: ['decl', 'are']\n",
      "Verb: destroy -> Tokens: ['destroy']\n",
      "Verb: devour -> Tokens: ['dev', 'our']\n",
      "Verb: die -> Tokens: ['die']\n",
      "Verb: empty -> Tokens: ['empty']\n",
      "Verb: enter -> Tokens: ['enter']\n",
      "Verb: faint -> Tokens: ['f', 'aint']\n",
      "Verb: fall -> Tokens: ['fall']\n",
      "Verb: fill -> Tokens: ['fill']\n",
      "Verb: flutter -> Tokens: ['fl', 'utter']\n",
      "Verb: freeze -> Tokens: ['free', 'ze']\n",
      "Verb: go -> Tokens: ['go']\n",
      "Verb: hit -> Tokens: ['hit']\n",
      "Verb: increase -> Tokens: ['incre', 'ase']\n",
      "Verb: jog -> Tokens: ['j', 'og']\n",
      "Verb: jump -> Tokens: ['jump']\n",
      "Verb: kill -> Tokens: ['kill']\n",
      "Verb: laugh -> Tokens: ['laugh']\n",
      "Verb: melt -> Tokens: ['m', 'elt']\n",
      "Verb: murmur -> Tokens: ['mur', 'mur']\n",
      "Verb: near -> Tokens: ['near']\n",
      "Verb: nibble -> Tokens: ['n', 'ibble']\n",
      "Verb: open -> Tokens: ['open']\n",
      "Verb: pour -> Tokens: ['pour']\n",
      "Verb: proclaim -> Tokens: ['pro', 'claim']\n",
      "Verb: propose -> Tokens: ['pro', 'pose']\n",
      "Verb: remove -> Tokens: ['remove']\n",
      "Verb: rise -> Tokens: ['rise']\n",
      "Verb: roll -> Tokens: ['roll']\n",
      "Verb: rub -> Tokens: ['rub']\n",
      "Verb: run -> Tokens: ['run']\n",
      "Verb: say -> Tokens: ['say']\n",
      "Verb: scour -> Tokens: ['sc', 'our']\n",
      "Verb: scream -> Tokens: ['sc', 'ream']\n",
      "Verb: scribble -> Tokens: ['scrib', 'ble']\n",
      "Verb: scrub -> Tokens: ['sc', 'rub']\n",
      "Verb: shout -> Tokens: ['sh', 'out']\n",
      "Verb: spin -> Tokens: ['spin']\n",
      "Verb: sweep -> Tokens: ['swe', 'ep']\n",
      "Verb: swim -> Tokens: ['sw', 'im']\n",
      "Verb: walk -> Tokens: ['walk']\n",
      "Verb: whisper -> Tokens: ['wh', 'is', 'per']\n",
      "Verb: wipe -> Tokens: ['w', 'ipe']\n",
      "Verb: yell -> Tokens: ['y', 'ell']\n",
      "Verbs not tokenized as single tokens:\n",
      "['admit', 'approach', 'arrive', 'bellow', 'dance', 'declare', 'devour', 'faint', 'flutter', 'freeze', 'increase', 'jog', 'melt', 'murmur', 'nibble', 'proclaim', 'propose', 'scour', 'scream', 'scribble', 'scrub', 'shout', 'sweep', 'swim', 'whisper', 'wipe', 'yell']\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "import re\n",
    "\n",
    "\n",
    "# Load RoBERTa tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
    "model = RobertaForMaskedLM.from_pretrained(\"roberta-large\")\n",
    "\n",
    "# List of verbs\n",
    "fname = \"C:/Users/pcass/manres2vec/data/english_manner_result.csv\"\n",
    "fh = open(fname, \"r\")\n",
    "content = fh.readlines()\n",
    "rootlist = []\n",
    "p = re.compile(\"^(.*?),\")\n",
    "for line in content:\n",
    "    if p.match(line): #for English (no freq counts)\n",
    "        rootlist.append(p.match(line).group(1))\n",
    "# Get verbs that are not tokenized as single words\n",
    "non_single_token_verbs = get_non_single_token_verbs_verbose(model, tokenizer, rootlist)\n",
    "\n",
    "print(\"Verbs not tokenized as single tokens:\")\n",
    "print(non_single_token_verbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86291127-7896-43e0-b65d-45130121cf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb: eat -> Tokens: ['eat']\n",
      "Verb: admit -> Tokens: ['admit']\n",
      "Verb: approach -> Tokens: ['approach']\n",
      "Verb: arrive -> Tokens: ['arrive']\n",
      "Verb: bash -> Tokens: ['bash']\n",
      "Verb: bellow -> Tokens: ['bell', '##ow']\n",
      "Verb: break -> Tokens: ['break']\n",
      "Verb: clean -> Tokens: ['clean']\n",
      "Verb: clear -> Tokens: ['clear']\n",
      "Verb: come -> Tokens: ['come']\n",
      "Verb: cover -> Tokens: ['cover']\n",
      "Verb: dance -> Tokens: ['dance']\n",
      "Verb: declare -> Tokens: ['declare']\n",
      "Verb: destroy -> Tokens: ['destroy']\n",
      "Verb: devour -> Tokens: ['dev', '##our']\n",
      "Verb: die -> Tokens: ['die']\n",
      "Verb: empty -> Tokens: ['empty']\n",
      "Verb: enter -> Tokens: ['enter']\n",
      "Verb: faint -> Tokens: ['faint']\n",
      "Verb: fall -> Tokens: ['fall']\n",
      "Verb: fill -> Tokens: ['fill']\n",
      "Verb: flutter -> Tokens: ['flutter']\n",
      "Verb: freeze -> Tokens: ['freeze']\n",
      "Verb: go -> Tokens: ['go']\n",
      "Verb: hit -> Tokens: ['hit']\n",
      "Verb: increase -> Tokens: ['increase']\n",
      "Verb: jog -> Tokens: ['jo', '##g']\n",
      "Verb: jump -> Tokens: ['jump']\n",
      "Verb: kill -> Tokens: ['kill']\n",
      "Verb: laugh -> Tokens: ['laugh']\n",
      "Verb: melt -> Tokens: ['melt']\n",
      "Verb: murmur -> Tokens: ['murmur']\n",
      "Verb: near -> Tokens: ['near']\n",
      "Verb: nibble -> Tokens: ['ni', '##bble']\n",
      "Verb: open -> Tokens: ['open']\n",
      "Verb: pour -> Tokens: ['pour']\n",
      "Verb: proclaim -> Tokens: ['pro', '##claim']\n",
      "Verb: propose -> Tokens: ['propose']\n",
      "Verb: remove -> Tokens: ['remove']\n",
      "Verb: rise -> Tokens: ['rise']\n",
      "Verb: roll -> Tokens: ['roll']\n",
      "Verb: rub -> Tokens: ['rub']\n",
      "Verb: run -> Tokens: ['run']\n",
      "Verb: say -> Tokens: ['say']\n",
      "Verb: scour -> Tokens: ['sc', '##our']\n",
      "Verb: scream -> Tokens: ['scream']\n",
      "Verb: scribble -> Tokens: ['sc', '##ri', '##bble']\n",
      "Verb: scrub -> Tokens: ['scrub']\n",
      "Verb: shout -> Tokens: ['shout']\n",
      "Verb: spin -> Tokens: ['spin']\n",
      "Verb: sweep -> Tokens: ['sweep']\n",
      "Verb: swim -> Tokens: ['swim']\n",
      "Verb: walk -> Tokens: ['walk']\n",
      "Verb: whisper -> Tokens: ['whisper']\n",
      "Verb: wipe -> Tokens: ['wipe']\n",
      "Verb: yell -> Tokens: ['yell']\n",
      "BERT - Verbs not tokenized as single tokens:\n",
      "['bellow', 'devour', 'jog', 'nibble', 'proclaim', 'scour', 'scribble']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer_bert = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "model_bert = BertForMaskedLM.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "# List of verbs\n",
    "\n",
    "# Check which verbs are not tokenized as single tokens by BERT\n",
    "non_single_token_verbs_bert = get_non_single_token_verbs_verbose(model_bert, tokenizer_bert, rootlist)\n",
    "\n",
    "print(\"BERT - Verbs not tokenized as single tokens:\")\n",
    "print(non_single_token_verbs_bert)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
